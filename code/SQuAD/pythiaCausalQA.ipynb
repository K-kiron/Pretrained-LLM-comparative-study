{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9454394b-7e7c-47a0-b7a6-b00f128ea5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import numpy as np\n",
    "from transformers import MambaForCausalLM, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629261c3-1515-42c7-9ef8-c3e76b8318b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "model_id = \"EleutherAI/pythia-160m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "# special_tokens_dict = {\"cls_token\": \"<CLS>\", \"sep_token\": \"<SEP>\"}\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#       (str(\"[SEP]\"), 50278), (str(\"[CLS]\"), 50277)\n",
    "# )\n",
    "\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "datasets = load_dataset(\"squad\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646fd95a-d3ef-49d2-9122-199b936ae925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568705b366924e6fbc243eb3f72df0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4229041022413da78dbc990278b898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87428\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10515\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def length_filter(example):\n",
    "    # Calculate the total length of question, context, and answers (assuming only one answer per question)\n",
    "    total_length = len(tokenizer(f\"{example['context']}\\n\\nQ: {example['question']}\\nA: {example['answers']['text'][0]}<|endoftext|>\")[\"input_ids\"])\n",
    "    return total_length <= 512\n",
    "\n",
    "datasets = datasets.filter(length_filter)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f71a3f2-2cbd-4e65-852b-9ed5bce0a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# model.config.keys_to_ignore_at_inference = [\"cache_params\", \"hidden_states\"]\n",
    "model = model.to(\"cuda:0\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9cd1cf-dcdd-4a09-b748-a22ae560c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"Pythia-finetuned-squadCausal/Final\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Pythia-finetuned-squadCausal/Final\")\n",
    "# model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a37da1-e2c0-49c9-b72d-4392537d14f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce25f4f8b8e3482fb93eee98f42f9836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 10515\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'guess'],\n",
      "    num_rows: 10515\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def prepare_validation_features(example):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    # tokenized_example = tokenizer(\n",
    "    #     f\"{example['context']}\\n\\nQ: {example['question']}\\nA:\",\n",
    "    #     max_length=max_length,\n",
    "    #     truncation=True,\n",
    "    # )\n",
    "    \n",
    "    text = f\"{example['context']}\\n\\nQ: {example['question']}\\nA:\"\n",
    "    input_ids = torch.LongTensor([tokenizer.encode(text)]).cuda()\n",
    "\n",
    "    # example[\"input_ids\"] = tokenized_example[\"input_ids\"]\n",
    "    # input_ids = torch.LongTensor([tokenized_example[\"input_ids\"]]).cuda()\n",
    "    out = model.generate(input_ids, max_length=max_length, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "    decoded = tokenizer.batch_decode(out)[0]\n",
    "    cleaned = decoded.replace(text, \"\")\n",
    "    cleaned = cleaned.replace(\"<|endoftext|>\", \"\")\n",
    "    guess = cleaned.split(\"\\n\\n\")[0].strip()\n",
    "    example[\"guess\"] = guess\n",
    "   \n",
    "    return example\n",
    "\n",
    "tokenized_validsets = datasets[\"validation\"].map(prepare_validation_features, batched=False)\n",
    "print(datasets[\"validation\"])\n",
    "print(tokenized_validsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb29c197-b271-4a19-ae5c-4c96714600e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcc635d184242d4945e5fe115d49399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8823a5322ada4ed183399bc3dc209765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.24726581074655254, 'f1': 5.387292106209826}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "squad_metric = load(\"squad\")\n",
    "predictions = [{\"id\": ex[\"id\"], 'prediction_text': ex[\"guess\"]} for ex in tokenized_validsets]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in tokenized_validsets]\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift6135-conda-env",
   "language": "python",
   "name": "ift6135-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
